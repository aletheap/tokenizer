{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# import the right version of tqdm for where we are running\n",
    "try:\n",
    "    env_type = get_ipython().__class__.__name__ \n",
    "except NameError:\n",
    "    env_type = 'terminal'\n",
    "if env_type == 'ZMQInteractiveShell':\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda')\n",
    "# Force cpu because it's much faster than mps.\n",
    "# I have no ide why, but such is life.\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    torch.set_default_device('mps')\n",
    "else:\n",
    "    torch.set_default_device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file: str = \"vocab.txt\"  # path to output vocab file\n",
    "overwrite: bool = True  # overwrite existing vocab file\n",
    "max_vocab_size: int = 50_000  # maximum size of vocab\n",
    "max_token_length: int = 40  # maximum length of token\n",
    "random_seed = 42\n",
    "\n",
    "#max_examples = 36_718\n",
    "max_examples = 100\n",
    "\n",
    "if not overwrite:\n",
    "    assert not os.path.exists(vocab_file), f\"{vocab_file} already exists\"\n",
    "assert max_vocab_size > 0, f\"max_vocab_size must be positive, got {max_vocab_size}\"\n",
    "assert max_token_length > 0, f\"max_token_length must be positive, got {max_token_length}\"\n",
    "max_entropy = float(np.log2(max_vocab_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset to calculate token frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ds_full = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "ds_full = load_dataset(\"graelo/wikipedia\", \"20230901.en\", split=\"train\")\n",
    "dl_full = ds_full.to_iterable_dataset().shuffle(seed=random_seed, buffer_size=10_000)\n",
    "\n",
    "ds = []\n",
    "with tqdm(total=max_examples, desc=\"loading dataset\", unit='example') as pb:\n",
    "    for i, ex in enumerate(dl_full):\n",
    "        if i >= max_examples:\n",
    "            break\n",
    "        ds.append(bytes(ex['text'], 'utf-8'))\n",
    "        pb.update(1)\n",
    "        pb.refresh()\n",
    "\n",
    "assert len(ds) == max_examples, f\"expected {max_examples} examples, got {len(ds)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(c, dim=None):\n",
    "    \"\"\"Converts a Counter or dict to a probability distribution\"\"\"\n",
    "    if isinstance(c, (dict, Counter)):\n",
    "        total = sum(c.values())\n",
    "        p = {k: v / total for k, v in c.items()}\n",
    "    else:\n",
    "        p = torch.tensor(c)\n",
    "        p = p / p.sum(dim=dim, keepdim=True)\n",
    "    return p\n",
    "    \n",
    "def entropy(c, dim=None, keepdim=False):\n",
    "    \"\"\"Computes the entropy of a Counter or dict\"\"\"\n",
    "    p = normalize(c, dim=dim)\n",
    "    if isinstance(c, (dict, Counter)):\n",
    "        p = torch.tensor(list(p.values()))\n",
    "    p = p[p > 0]\n",
    "    return -torch.sum(p * torch.log2(p).nan_to_num(), dim=dim, keepdim=keepdim)\n",
    "\n",
    "def make_inital_vocab(ds=ds):\n",
    "    \"\"\"Creates an initial vocab from a dataset\"\"\"\n",
    "    c = Counter({bytes([i]):0 for i in range(256)})\n",
    "    for example in tqdm(ds, desc=f\"finding all 1-byte token frequencies\", smoothing=0, unit=\"example\", leave=True):\n",
    "        for i in example:\n",
    "            #assert isinstance(i, int) and i <256, f\"expected 1-byte tokens, got {repr(i)}\"\n",
    "            c.update({bytes([i]): 1 })\n",
    "            #c[bytes([i])] = c.get(bytes([i]), 0) + 1\n",
    "    return c\n",
    "\n",
    "def get_freqs(tokens, ds=ds, progress=False):\n",
    "    \"\"\"Finds the frequency of each token in tokens in the dataset ds\"\"\"\n",
    "    #tqdm.write(f\"finding frequency of {len(tokens)} tokens...\")\n",
    "    freqs = defaultdict(int)\n",
    "    if progress:\n",
    "        ds = tqdm(ds, total=len(ds), desc=f\"finding frequency of {len(tokens)} tokens\", smoothing=0, unit=\"example\")\n",
    "    for ex in ds:\n",
    "        for t in tokens:\n",
    "            freqs[t] += ex.count(t)\n",
    "    return freqs\n",
    "\n",
    "def find_best_merge(all_tokens, all_freqs, a_token, b_tokens, b_freqs, merge_tokens, merge_freqs):\n",
    "    \"\"\"Finds the best b_token to merge with a given a_token to maximize the entropy of the frequencies of the resulting token set\"\"\"\n",
    "    all_freqs = torch.tensor(all_freqs) \n",
    "    merge_freqs = torch.tensor(merge_freqs)\n",
    "\n",
    "    freqs_t = normalize(all_freqs).unsqueeze(0) * torch.ones(len(merge_freqs), len(all_freqs))\n",
    "    freqs_t = torch.cat([freqs_t, merge_freqs.unsqueeze(1)], dim=1)\n",
    "    assert freqs_t.shape == (len(merge_freqs), len(all_freqs)+1), f\"{len(all_freqs)+1=}, {freqs_t.shape=}, {len(merge_freqs)=} {merge_freqs.unsqueeze(0).shape=}\"\n",
    "    at_idx = torch.tensor([all_tokens.index(a_token)])\n",
    "    freqs_t[:, at_idx] = freqs_t[:, at_idx] - merge_freqs.unsqueeze(1)\n",
    "    bt_idxes = torch.tensor([all_tokens.index(bt) for bt in b_tokens])\n",
    "    freqs_t[:, bt_idxes] = freqs_t[:, bt_idxes] - merge_freqs.unsqueeze(1)\n",
    "    #for m_idx, (bt, bf, mt, mf) in enumerate(zip(b_tokens, b_freqs, merge_tokens, merge_freqs)):\n",
    "    #    bt_idx = all_tokens.index(bt)\n",
    "    #    freqs_t[m_idx, bt_idx] -= float(mf)\n",
    "    #    freqs_t[m_idx, at_idx] -= float(mf)\n",
    "    freqs_t = normalize(freqs_t, dim=1)\n",
    "    h = entropy(freqs_t, dim=-1)\n",
    "    best_idx = int(torch.argmax(h))\n",
    "    \n",
    "    b_token = bytes(b_tokens[best_idx])\n",
    "    b_freq = float(b_freqs[best_idx])\n",
    "    merge_token = bytes(merge_tokens[best_idx])\n",
    "    merge_ffreq = float(merge_freqs[best_idx])\n",
    "    \n",
    "    return (b_token, b_freq, merge_token, merge_ffreq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize vocabulary with all 256 1-byte tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = make_inital_vocab()\n",
    "\n",
    "token_freqs_list = list(reversed(vocab.most_common()))\n",
    "freqs = torch.tensor([x[1] for x in token_freqs_list])\n",
    "x = torch.arange(len(freqs))\n",
    "print(f\"least common 1-byte tokens: {token_freqs_list[:10]}\")\n",
    "print(f\"most common 1-byte tokens: {list(reversed(token_freqs_list[-10:]))}\")\n",
    "print(f\"total 1-byte tokens: {len(vocab.values())}\")\n",
    "print(f\"{max_vocab_size=:,}, {max_token_length=:,} number of 1-char tokens={len(vocab):,}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize distribution of token freqencies in initial vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ascii byte ranges\n",
    "punct_idx = torch.tensor(list(range(32,48)) + list(range(58,65)) + list(range(91,97)) + list(range(123,127)))\n",
    "digit_idx = torch.tensor(list(range(48,58)))\n",
    "lower_idx = torch.tensor(list(range(97,123)))\n",
    "upper_idx = torch.tensor(list(range(65,91)))\n",
    "control_idx = torch.tensor(list(range(0,32)))\n",
    "other_idx = torch.tensor(list(range(127,256)))\n",
    "\n",
    "# plot frequencies\n",
    "plt.scatter(x[punct_idx].tolist(), freqs[punct_idx].tolist(), marker='.', color='orange', label='ascii punctuation')\n",
    "plt.plot(x[other_idx].tolist(), freqs[other_idx].tolist(), color='black', label='ascii control')\n",
    "plt.plot(x[digit_idx].tolist(), freqs[digit_idx].tolist(), color='red', label='ascii digits')\n",
    "plt.plot(x[upper_idx].tolist(), freqs[upper_idx].tolist(), color='green', label='ascii uppercase')\n",
    "plt.plot(x[lower_idx].tolist(), freqs[lower_idx].tolist(), color='blue', label='ascii lowercase')\n",
    "plt.plot(x[other_idx].tolist(), freqs[other_idx].tolist(), color='purple', label='non-ascii')\n",
    "plt.yscale('log')\n",
    "plt.title(f'frequency of each 1-byte token\\nvocab entropy: {entropy(freqs):.1f} / {np.log2(256):.1f} / {max_entropy:.1f} bits')\n",
    "plt.xlabel('byte')\n",
    "plt.ylabel('byte frequency (log scale)')\n",
    "plt.xticks(list(range(0, 257, 32))) #, rotation=90, labels=xtick_labels)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "if env_type == 'ZMQInteractiveShell':\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.savefig('1-byte-token-frequencies.png', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge to increase entropy of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_freqs_cache = {t: f for t, f in vocab.items()}\n",
    "\n",
    "raw_freqs_cache.update(get_freqs([t1+t2 for t1 in vocab for t2 in vocab], progress=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc(f, max_h=max_entropy):\n",
    "    return f\"entropy: (h={entropy(f):.02f}/{np.log2(len(f)):.02f}/{max_h:.02f} bits) \"\n",
    "\n",
    "x_time = []\n",
    "y_freqs = []\n",
    "\n",
    "\n",
    "with tqdm(desc=f\"tokens: {vocab.most_common(3)=} \", total=max_vocab_size, initial=len(vocab), unit=\"token\", position=0, leave=True) as tokens_pb:\n",
    "    with tqdm(desc=desc(vocab), total=round(max_entropy, 2), initial=round(float(entropy(vocab)), 2), unit=\"bit\", smoothing=1, position=1, leave=True) as entropy_pb:\n",
    "        while len(vocab) < max_vocab_size:\n",
    "            x_time.append(time.time())\n",
    "            y_freqs.append(list(vocab.values()))\n",
    "            median_freq = vocab.most_common()[len(vocab)//2][1]\n",
    "            mergable_tokens_freqs = [(t,f) for t,f in vocab.most_common() if f >= 2]\n",
    "            mergable_tokens, mergable_freqs = list(zip(*mergable_tokens_freqs))\n",
    "            new_merged_tokens = [t1 + t2 for t1 in mergable_tokens for t2 in mergable_tokens if t1+t2 not in raw_freqs_cache]\n",
    "            raw_freqs_cache.update(get_freqs(new_merged_tokens))\n",
    "\n",
    "            no_mergable_tokens = True\n",
    "            for at, af in mergable_tokens_freqs:\n",
    "                af = int(af)\n",
    "                rtfmtf = [(bt, bf, at+bt, raw_freqs_cache[at+bt]) for bt, bf in mergable_tokens_freqs if (at + bt not in vocab)]\n",
    "                ltfmtf = [(bt, bf, bt+at, raw_freqs_cache[bt+at]) for bt, bf in mergable_tokens_freqs if (bt + at not in vocab)]\n",
    "\n",
    "                btfmtf = rtfmtf + ltfmtf        \n",
    "                btfmtf = [(bt, bf, mt, mf) for bt, bf, mt, mf in btfmtf if mf < af] #.87*af]\n",
    "                btfmtf = [(bt, bf, mt, mf) for bt, bf, mt, mf in btfmtf if mf < bf] #.87*bf]\n",
    "                btfmtf = [(bt, bf, mt, mf) for bt, bf, mt, mf in btfmtf if mf > 0] #.87*bf]\n",
    "                btfmtf = [(bt, bf, mt, mf) for bt, bf, mt, mf in btfmtf if len(mt) <= max_token_length]\n",
    "\n",
    "                if btfmtf:\n",
    "                    all_tokens, all_freqs = list(zip(*vocab.most_common()))\n",
    "                    (bt, bf, mt, mf) = find_best_merge(all_tokens, all_freqs, at, *zip(*btfmtf))\n",
    "                    no_mergable_tokens = False\n",
    "                    break\n",
    "                    \n",
    "            if no_mergable_tokens:\n",
    "                print(\"no mergable tokens\")\n",
    "                break\n",
    "\n",
    "            vocab[mt] = int(mf)\n",
    "            vocab[at] -= int(mf)\n",
    "            vocab[bt] -= int(mf)\n",
    "            for t, f in reversed(vocab.most_common()):\n",
    "                if (f <= 0) and (len(t) > 1):\n",
    "                    del vocab[t]\n",
    "                else:\n",
    "                    break\n",
    "            tokens_pb.n = len(vocab)\n",
    "            tokens_pb.desc = f\"tokens: {vocab.most_common(3)=} \"\n",
    "            entropy_pb.desc = f\"entropy: (best possible: {round(np.log2(len(vocab)), 2)})\"\n",
    "            entropy_pb.n = round(float(entropy(vocab)), 1)\n",
    "            tokens_pb.refresh()\n",
    "            entropy_pb.refresh()\n",
    "            x_time.append(time.time())\n",
    "            y_freqs.append(list(vocab.values()))\n",
    "            #pb.desc = desc(vocab)\n",
    "\n",
    "x_time.append(time.time())\n",
    "y_freqs.append(list(vocab.values()))\n",
    "with open(vocab_file, 'w') as f:\n",
    "    f.write(\"id, token, freq\\n\")\n",
    "    for i, (t, f) in enumerate(vocab.most_common()):\n",
    "        f.write(f\"{i}, {repr(t)}, f\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize the vocabulary development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/9401658/how-to-animate-a-scatter-plot\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "class Animated(object):\n",
    "    def __init__(self, x_time, y_freqs, frame_rate):\n",
    "        self.x_time, self.y_freqs = x_time, y_freqs\n",
    "        self.stream = self.data_stream()\n",
    "        self.frame_rate = frame_rate\n",
    "        self.ymin = 1\n",
    "        self.ymax = max([int(f) for f in self.y_freqs[0]])\n",
    "\n",
    "        # Setup the figure and axes...\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "        # Then setup FuncAnimation.\n",
    "        self.ani = animation.FuncAnimation(self.fig, self.update, interval=5, \n",
    "                                          save_count=len(self.x_time), blit=True) # init_func=self.update, \n",
    "\n",
    "    def data_stream(self):\n",
    "        for t_raw, y_raw in tqdm(zip(self.x_time, self.y_freqs), total=len(self.x_time), desc=\"generating data stream\", unit=\"frame\", leave=True):\n",
    "            t = time.strftime('%Y-%m-%d @ %I:%M:%S %Z', time.gmtime(t_raw)) # + str(t_raw % 60)\n",
    "            y = sorted(y_raw)\n",
    "            x = np.arange(len(y))\n",
    "            yield (t, x, y)\n",
    "\n",
    "    def update(self, i=0):\n",
    "        try:\n",
    "            t, x, y = next(self.stream)\n",
    "        except StopIteration:\n",
    "            return []\n",
    "        for line in self.ax.lines:\n",
    "            line.remove()\n",
    "        self.plot = self.ax.plot(x, y, color='black')\n",
    "        self.ax.set_title(f\"{t}\\nentropy: {entropy(y):.01f} / {np.log2(len(y)):.01f} / {max_entropy:.01f} bits , {len(y)} tokens\")\n",
    "        self.ax.set_ylabel('num occurences')\n",
    "        self.ax.set_xlabel('token id')\n",
    "        self.ax.set_yscale('log')\n",
    "        #self.ax.set_xscale('log')\n",
    "        #self.ax.set_ylim(0, max(y))\n",
    "        self.ax.set_xlim(0, max_vocab_size)\n",
    "        self.ax.set_ylim(self.ymin, self.ymax)\n",
    "        #plt.show()\n",
    "        return self.plot\n",
    "\n",
    "\n",
    "framerate = 120\n",
    "a = Animated(x_time[:120], y_freqs[:120], framerate)\n",
    "plt.show()\n",
    "a.ani.save('test.mp4', fps=120, extra_args=['-vcodec', 'libx264'])\n",
    "#a.ani.save('test.gif', fps=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
